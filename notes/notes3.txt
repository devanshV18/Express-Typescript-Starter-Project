LOGGING

It keeps track of events in a request or a process in ourr system. It is done to track the procedures, events occured while processing the request or any process. It helps us like clues and leads to understand what happened with the system in case anything goes wrong.

 Why not use console.log thru out our implementation?

 1. non persistence ->> console.log only appear for current terminal session.
 2. log storage
 3. not every flow will have error and require error logging, rather we need multiple types og log like info log, warning log, error log etc. which demands an individual service that helps us intergrate production grade logging.

 logging library - winston.

 winston works by defining/creating a logger object -> configuring it as per our needs by setting different properties and finally exporting the configured logger object. This object can be imported and used through out the project for logging.

 ## Important Properties in Winston logging

 1. Format -> It defines when a log comes up due to an info, warning or error, What should the log actually write.

 2. Transports -> It defines how the logs are to be processed in terms of their storage, are these just going to be printed on terminal or also added to ant file I/o or any db is configured here.

 3. levels -> type of logs like info, warning, error, fatal etc.

 he recommended way to use winston is to create your own logger. The simplest way to do this is using winston.createLogger:

const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.json(),
  defaultMeta: { service: 'user-service' },
  transports: [
    //
    // - Write all logs with importance level of `error` or higher to `error.log`
    //   (i.e., error, fatal, but not other levels)
    //
    new winston.transports.File({ filename: 'error.log', level: 'error' }),
    //
    // - Write all logs with importance level of `info` or higher to `combined.log`
    //   (i.e., fatal, error, warn, and info, but not trace)
    //
    new winston.transports.File({ filename: 'combined.log' }),
  ],
});

//
// If we're not in production then log to the `console` with the format:
// `${info.level}: ${info.message} JSON.stringify({ ...rest }) `
//
if (process.env.NODE_ENV !== 'production') {
  logger.add(new winston.transports.Console({
    format: winston.format.simple(),
  }));
} 

//syntax -> logger.level("message" + other configs (if any))


//Requests to our server will navigate thru multiple layers like controller, routing, service, validation layer etc.
And in addition to that at a single time, multiple requests will be hitting the server parallely. 

Problem Statement -> If user 1 makes a request and logs come up for the user 1 from the loggers in place throughout our system in various layers. Parallely user 2 also makes request so logs for user 2 will also come up. In this case its not possible  that our logging mechanism will have all logs of user 1 is stored in db first and then all logs of user 2. The log storage will be such that logs will interleave i.e. a few logs of user 1 and then a few logs of user 2. it will be sort of mixed in a line. 

In such case to solve this problem there is a concept called co-relation ID.

co-relation ID is a unique id added to (every log of a single request). Suppose a request comes up, it hits router -> validation -> controller -> service -> repo .. so on . So a request navigates thru so many layers performing multiple functions to serve the request but it is accounted as a single request. What we can do is that we can generate a unique id for this particular request. 

The benefit of this corelation id is that we can track entire flow of a single request through its logs by looking at all logs of that particular request.

So we want to setup a mech of adding a unique corealtion id that identifies a single request and all the logs of this single request will also be attached with that co-relation ID.


##  -> IMP
A basic approach would be to create a middleware that is invoked before the routes and generates the uuid and using the req object, we append the uuid in the req body before it hits any route and from there on whenever a log comes up we can pass the uuid to be attached with the log.

## -> IMP
Corner case with this approach -> Suppose we have any peer function (like calculating interest amount or any such helper fxn that helps in between the flow of our request processing.) which doesn't require the entire req obj rather it need a particular data or a few values for its computation to be done and then it returns the result and we move forward with the request. But if that helper/peer fxn has some logs to be entered, we will have to either pass the entire req body or the uuid separately to such functions. Hence the problem is we need to manually pass uuid to such functions.
 
## -> IMP
Another corner case -> Suppose we have some background cron jobs that invoke or run in every 10 mins such as payment remainders, mailers etc, No one actually pass a req to invoke such processes, it itself works on time intervals, so how the logs in such functions and processes be attached to a uuid/co-relation ID when they simply dont belong to any particular request.


So this approach works fine for 80-90 percent of all over the api network request where no peer function is required but it misses all the background and other cron processes which are not associated to any request.

## How do we solve it?
Basically we need to get a correlation id attached with this background/cron process.
Solution = using Async Storage.

Async storage helps us to put a unique id or correlation id in any asynchronous request that we have such as an API req, a background job etc. using Asynchronous Context tracking feature provided in nodejs.

# What is Async Local Storage? 
ANS -> For one async operation/request that arises in our project, we have a storage availabe just for that particular async request. 

So basically we want to attach all the async operations such as incoming http api req, processes in validation, controller, service layer, background jobs with a correlation id so that each log can be associated to a single request using the correlation id.

So we can create a async local storage and add attach a unique id to this async local storage, Now anytime a logger is hit i.e. we are in a async request doesn't matter a http api req, an helper fxn, a background job etc every request is going to have a unique correlation id attached to it. (Will be more clear with the implementation.)

//NOTE - async store gets created once when the server starts but it has unique data for each request.

How are we implementing it :- 

First we are attaching a uuid correlation id with our asyncLocalStorage in the correlation.middleware. Using the run function we call next() which basically runs next() within the current async context -> so any time when the logger is hit we ahve configured the logger to fetch the correlationId using getCorrelationId function in the request.helpers.ts file and it fetched the correlationId of the current local storage and displays it in the log.


Basically every request is now being handled as one async context and we have added correlationId in the async store of that async context (in correlation.middleware.ts) for each async context we have one correaltionId. Hence when any middleware invokes the logger or uses the logger it calls getcorrelationId function fetches the asyncStore which gives correaltion id for the current async context and hence we will be able to attach correlation id to all the async operations.

asyncLocalStorage.run( () => {} )

run( ) = Runs the function (callback argument passed) synchronously within a current async context and retrun its return value.